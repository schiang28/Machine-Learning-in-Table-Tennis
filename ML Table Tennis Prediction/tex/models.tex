\section{Models} \label{sec:models}

%\denes{This is where we can still cut out quite a lot if we want to}
This paper uses Scikit-Learn's model implementations \cite{pedregosa2011scikit}.

\subsection{Logistic Regression}
The logistic function $\sigma(t)$ is defined as:
\begin{equation}
    \sigma(x) = \frac{1}{1+e^{-x}}
\end{equation}

The logistic function maps input value $x$ to a value between 0 and 1, allowing the output to be interpreted as a probability. Probabilities over $0.5$ can be interpreted as the player winning the match.  Training minimises the \textit{logistic loss} function:
\begin{equation}
    (p) = -\frac{1}{n} \sum_{i=1}^n p_i \log(y_i) + (1-p_i)\log(1-y_i), 
\end{equation}
where $n$ is number of matches, $p_i$ is the predicted probability of a player winning match $i$, and  $y_i$ is the actual outcome \cite{hazan2014logistic}. 
%$$
%\begin{gathered}
%    n = \text{number of matches} \\
%    p_i = \text{predicted probability of a player winning match $i$} \\
%    y_i = \text{outcome of match $i$},
%\end{gathered}
%$$

%where a loss function measures the disparity between observations and their estimated fits .

\subsection{Random Forest}
A random forest classifiers consist of an ensemble of simpler decision trees $\{h(\textbf{x},\theta_k),\ k=1,...\}$, where the $\{\theta_k\}$ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input $\textbf{x}$. For the $k$th tree, a random vector $\theta_k$ is generated and a tree is grown using $\theta_k$ and the training set, to produce a classifier $h(\textbf{x}, \theta_k)$ \cite{breiman2001random}. During inference  the output of a random forest model is decided by a majority vote. Decision trees tend to be simpler to interpret and quicker to train.

\subsection{Support Vector Machines (SVM)}
SVMs identify the optimal hyperplane in the multi-dimensional feature space that separates data points into the two target classes (win, lose). During training, the marginal distance between this decision boundary and the instances closest to the boundary is maximized. %The existence of a decision boundary can allow for any detection of miss-classification. 
SVMs have a choice of \textit{kernels}, including linear, polynomial, sigmoid or a radial basis function.

SVMs have been used for  \textit{tennis} match predictions \cite{cornman2017machine}.

\subsection{Multilayer Perceptron Neural Networks (MLP)}
An MLP is a simple artificial neural network consisting of an input layer (features), an output layer (prediction class), and  one or more hidden layers in-between. Neurons in consecutive layers are connected (no connections within layers) \cite{noriega2005multilayer}, with each connection hacing an associated weight. Training an MLP involves adjustments of these weights using backpropagation to minimize the difference between model output and the desired output.
