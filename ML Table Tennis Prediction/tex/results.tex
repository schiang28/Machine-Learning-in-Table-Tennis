\subsection{Evaluating Models} \label{evalmodels}
%\todo{shorten this}
To compare the performance of different model predictions, we calculated the \textit{accuracy} of each model as
\begin{equation}
    \text{accuracy} = \frac{tp+tn}{tp+tn+fp+fn},
\end{equation}
where $tp$ and $tn$ are true positives and true negatives, and $fp$ and $fn$ are false positives and false negatives respectively. Additionally, detailed confusion matrices are also reported. %\cite{vanwinckelen2012estimating}. 

%\sophie{should figures 2 and 3 be kept?}
%\denes{No, but we could report all the confusion matrices for each model (possibly in the appendices).}
To get a more balanced idea about model performance, we also compute F1 scores as:
%Precision is defined as the proportion of true positives to the total number of predictions predicted positive, and is a percentage of returned results which are relevant. Recall is defined to be the proportion of true positives to the total number of actual positives, and is a percentage of relevant data which have been correctly classified \cite{buckland1994relationship}.
%\begin{equation}
\begin{gather}
    \text{precision} = \frac{tp}{tp+fp} \quad
 \text{recall} = \frac{tp}{tp+fn}\\
 \vspace{0.1cm} \notag \\
F1 = \frac{2\times\text{precision} \times \text{recall}}{\text{precision} + \text{recall}},
\end{gather}
%\end{equation}
which is effectively an F measure with $\beta=1$ \cite{sokolova2006beyond}.
%Both metrics are important to take into consideration, and ultimately we use the F1 measure, which can be interpreted as the harmonic mean between precision and recall \cite{buckland1994relationship}, and accuracy score for evaluation of each classifier.

% An issue associated with training models is the possibility for the model to \textit{overfit}.
To avoid over-fitting, we use 5-fold cross validation. The whole dataset was split in a training:validation:test set in a 18:72:10 ratio. 10\% of the original dataset was kept as a test set to validate hyperparameter tuning. The remaining 90\% of data was split in an 80:20 ratio for the 5-fold training; 80\% was used to train the model, the 20\% was used to optimize hyperparameters. %The model trains on the training set, and the validation set is used for optimising the hyperparameters of the model.
%Overfitting occurs when a model captures unwanted bias and noise in the data that it negatively impacts the performance of a model. The model corresponds to it's initial training data too well, and fails to predict unseen data reliably. In order to limit overfitting, a popular re-sampling technique called $k$-fold cross validation is used.
%In cross validation, the dataset is split into $k$ random subsets, known as folds, and one is selected as a test set for the model to test on, while the others are used as a training set for the model to train on. This is repeated $k$ times where a different subset of data is used as the test set each time, and the overall performance of the model is calculated as the average of accuracy scores for each iteration \cite{berrar2019cross}.

\subsection{Hyperparameter Tuning} \label{hpt}
We used a brute-force grid search to fine tune parameters of the model that are outside the usual training domain (hyperparameters) e.g. the number of trees in a random forest classifier.
%For each model, the \textit{hyperparameters}, parameters that are not optimised by the training algorithm e.g. the number of trees in a random forest classifier, were tuned manually by using a grid search to test different values.
%A grid search uses brute force to search the entire space for different hyperparameter configurations.
The best combination of hyperparameters for a model is determined by whichever has the highest accuracy on the validation set using 5-fold cross validation.

For logistic regression, the type of solver, penalty function and the $C$  terms value were adjusted. $C$ is a regularisation term; 
%\textit{Regularisation} prevents overfitting of training data by penalising large weights when training a logistic regression predictor, and the parameter $C$ mentioned is used to control the effect of this\cite{ahmadian1998regularisation}. 
the lower the value of $C$, the stronger the effect of regularisation. We found that  $C=1.0$, and `liblinear' solver resulted in the best average accuracy.

$l1$ regularisation penalises the sum of absolute values of weights, whereas $l2$ regularisation penalises the sum of squares of the weights \cite{ahmadian1998regularisation}. For our data, $l2$ regularisation gave the best results (Fig. \ref{fig:learningcurve}).
%\denes{Could we plot l1 vs l2 here? That would be quite convincing}.

For SVMs, the two main hyperparameters that were adjusted were the kernel type and penalty value $C$. Using a linear kernel and and $C=0.2$ gave the highest F1 score on the test set compared to other kernels. The learning curve for an SVM model using a linear kernel is shown in Figure \ref{fig:learningcurve}.

\begin{figure}[ht]
\includegraphics[width=8.5cm]{plots/learningcurves.pdf}
\caption{Logistic regression and linear kernel SVM Learning Curve.}
\label{fig:learningcurve}
\centering
\end{figure}


\begin{figure*}[h]
\centering
\includegraphics[width=18cm]{plots/confusionmatrices.pdf}
\vspace{-1em}
\caption{Confusion matrices comparing the predicted and actual outcomes of test cases for each trained model.}
%\denes{Please reduce outside margins and consider removing the legend}}

\label{fig:confusionmatrices}
\centering
\end{figure*}

\section{Experimental Results} \label{experresults}
Our main results are reported in Table~\ref{results} and \figref{confusionmatrices}.
%The test set is completely left out from the training process of the model, and is only used during evaluation. This is to see how well models generalise to unseen data which replicate new match data, as the test set is never used before evaluation. During evaluation, all other data is used for training the model (training and validation).
Both accuracy and F1 score are reported for the validation and test sets. The standard error for each score for the validation set is reported as a basis of defining uncertainty. The validation column shows that most models perform comparably with approx. 70\% accuracy. This value is also comparable to state-of-the-art metrics in \textit{tennis} match prediction.

F1 scores indicate that MLP Neural Networks (with a \textit{relu} activation) slightly over-perform their competitors, but the difference is not significant. The hidden layer size was set to 2 and the maximum number of iterations the solver iterates was chosen to be 200. The solver for weight optimization is set to `lbfgs', a quasi-Newton optimizer. The learning rate for scheduling weight updates is set to constant. However, the generic layered structure of a neural network has proven to be time consuming. Additionally, this technique is considered a ``black box'' technology, and finding out why a neural network has outstanding or even poor performance is  difficult \cite{noriega2005multilayer}.

On the test set, logistic regression seems to perform  noticeably better than other models. One interesting observation is that this is the only classifier that noticeably benefited the most from hyperparameter tuning. This might be partially due to the fact that we report the different kernels of SVMs separately in the table.

\begin{table}[ht]
\caption{Model performance comparing validation and test sets}
\label{results}
\centering
\setlength{\tabcolsep}{3pt}
\scalebox{1.05}{%
\begin{tabular}{ l|c c|c c }

\multirow{2}{4em}{Model} &
\multicolumn{2}{|c|}{Validation set} &
\multicolumn{2}{|c}{Test set} \\
\cline{2-5}
  & Acc & F1 & Acc & F1 \\

\hline \hline 
Logistic Regression & 0.699$\pm{0.024}$ & 0.705$\pm{0.023}$ & 0.722 & 0.706 \\
Random Forest & 0.677$\pm{0.032}$ & 0.688$\pm{0.033}$ & 0.667 & 0.684 \\
Support Vector Machine & & \\
$\rightarrow$ Linear & 0.696$\pm{0.029}$ & 0.690$\pm{0.035}$ & 0.639 & 0.629 \\
$\rightarrow$ RBF    & 0.700$\pm{0.025}$ & 0.677$\pm{0.034}$ & 0.667 & 0.600 \\
$\rightarrow$ Polynomial & 0.705$\pm{0.021}$ & 0.685$\pm{0.021}$ & 0.611 & 0.563\\
$\rightarrow$ Sigmoid & 0.705$\pm{0.017}$ & 0.690$\pm{0.019}$ & 0.694 & 0.621 \\
MLP Neural Network & 0.696$\pm{0.019}$ & 0.708$\pm{0.020}$ & 0.694 & 0.703\\
[1ex]
\hline
\end{tabular}}
\end{table}

\begin{figure}[ht]
\includegraphics[width=8.5cm]{plots/roccurves.pdf}
\caption{ROC Learning Curves.}
%\denes{As discussed, please replot without axis / bigger fonts. Or actually, why not do one single ROC plot (you have 4 colours anyway?}}
\vspace{-1em}
\label{fig:roc}
\centering
\end{figure}

Receiving operating characteristics (ROC) learning curves can be seen in \figref{roc}. It is clear that the kernel choice for SVM models make a great difference for our target application; the areas under the ROC curves are otherwise comparable for all other models.
%\figref{par} shows that the precision vs. recall trade-off is once again comparable for all models, except for the random forest classifier. While this seems like an interesting phenomenon, 

%ROC curves demonstrate the performance of a classification model for all classification thresholds. The closer the apex of the curve is to the upper left hand corner, the greater the model's discriminatory ability \cite{fan2006understanding}. This is plotted for each type of classifier, including the SVM model using a linear kernel. 
%For each trained model, we use it's accuracy score, F1 measure and the area under its ROC curve to evaluate overall performance\denes{do we report area as well anywhere?}.


%\begin{figure}[ht]
%\includegraphics[width=8.5cm]{plots/parcurve.pdf}
%\caption{Precision and Recall Curves.}
%\denes{As discussed, please replot without axis / bigger fonts or single plot}}

%\label{fig:par}
%\centering
%\end{figure}

