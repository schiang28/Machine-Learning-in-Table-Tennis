\subsection{Evaluating Models} \label{evalmodels}
%\todo{shorten this}
To compare the performance of different model predictions, we calculated the \textit{accuracy} of each model
\begin{equation}
    \text{accuracy} = \frac{tp+tn}{tp+tn+fp+fn},
\end{equation}
where $tp$ and $tn$ are true positives and true negatives, and $fp$ and $fn$ are false positives and false negatives respectively.  %\cite{vanwinckelen2012estimating}. 
%\sophie{should figures 2 and 3 be kept?}
%\denes{No, but we could report all the confusion matrices for each model (possibly in the appendices).}
To get a more balanced idea about model performance, we also compute F1 scores as:
%Precision is defined as the proportion of true positives to the total number of predictions predicted positive, and is a percentage of returned results which are relevant. Recall is defined to be the proportion of true positives to the total number of actual positives, and is a percentage of relevant data which have been correctly classified \cite{buckland1994relationship}.
%\begin{equation}
\begin{gather}
    \text{precision} = \frac{tp}{tp+fp} \quad
 \text{recall} = \frac{tp}{tp+fn}\\
 \vspace{0.0cm} \notag \\
F1 = \frac{2\times\text{precision} \times \text{recall}}{\text{precision} + \text{recall}},
\end{gather}
%\end{equation}
which is effectively an F measure with $\beta=1$ \cite{sokolova2006beyond}.
%Both metrics are important to take into consideration, and ultimately we use the F1 measure, which can be interpreted as the harmonic mean between precision and recall \cite{buckland1994relationship}, and accuracy score for evaluation of each classifier.

% An issue associated with training models is the possibility for the model to \textit{overfit}.
To avoid over-fitting, we used 5-fold cross validation. The  dataset was split in training:validation:test in a 72:18:10 ratio. 10\% of the original dataset was kept as a test set to validate hyperparameter tuning. The remaining 90\% of data was split in an 80:20 ratio for the 5-fold training; 80\% to train the model, 20\% to optimize hyperparameters. %The model trains on the training set, and the validation set is used for optimising the hyperparameters of the model.
%Overfitting occurs when a model captures unwanted bias and noise in the data that it negatively impacts the performance of a model. The model corresponds to it's initial training data too well, and fails to predict unseen data reliably. In order to limit overfitting, a popular re-sampling technique called $k$-fold cross validation is used.
%In cross validation, the dataset is split into $k$ random subsets, known as folds, and one is selected as a test set for the model to test on, while the others are used as a training set for the model to train on. This is repeated $k$ times where a different subset of data is used as the test set each time, and the overall performance of the model is calculated as the average of accuracy scores for each iteration \cite{berrar2019cross}.

\subsection{Hyperparameter Tuning} \label{hpt}
We used a brute-force grid search to fine tune parameters of the model that are outside the usual training domain (hyperparameters) e.g. the number of trees in a random forest classifier.
%For each model, the \textit{hyperparameters}, parameters that are not optimised by the training algorithm e.g. the number of trees in a random forest classifier, were tuned manually by using a grid search to test different values.
%A grid search uses brute force to search the entire space for different hyperparameter configurations.
The best combination of hyperparameters for a model is determined by whichever has the highest accuracy on the validation set using 5-fold cross validation.

For logistic regression, the type of solver, penalty function and the $C$  terms value were adjusted. $C$ is a regularisation term; 
%\textit{Regularisation} prevents overfitting of training data by penalising large weights when training a logistic regression predictor, and the parameter $C$ mentioned is used to control the effect of this\cite{ahmadian1998regularisation}. 
the lower the value of $C$, the stronger the effect of regularisation. We found that  $C=1.0$, and `liblinear' solver resulted in the best average accuracy. In terms of regularisation, L2 regularisation gave better results than L1 (Fig. \ref{fig:learningcurve}).

%\denes{Could we plot l1 vs l2 here? That would be quite convincing}.

For SVMs, the two main hyperparameters that were adjusted were the kernel type and penalty value $C$. Using a linear kernel and $C=0.2$ gave the highest F1 score on the test set compared to other kernels. The learning curve for an SVM model using a linear kernel is shown in Fig. \ref{fig:learningcurve}.

\begin{figure}[H]
\includegraphics[width=8.cm]{plots/chiang5.pdf}
\caption{Logistic regression and linear kernel SVM learning curves. The difference in F1 score for L1 and L2 regularisation is also illustrated.}
\label{fig:learningcurve}
\centering
\end{figure}

\section{Experimental Results} \label{experresults}
Our main results are reported in Table~\ref{results} and \figref{confusionmatrices}.
%The test set is completely left out from the training process of the model, and is only used during evaluation. This is to see how well models generalise to unseen data which replicate new match data, as the test set is never used before evaluation. During evaluation, all other data is used for training the model (training and validation).
Both accuracy and F1 score are reported for the validation and test sets. The standard error for each score for the validation set is reported as a basis of defining uncertainty. The validation column shows that most models perform comparably with approx. 70\% accuracy. This value is also comparable to state-of-the-art metrics in \textit{tennis} match prediction.

F1 scores indicate that MLP Neural Networks (with a \textit{relu} activation) slightly over-perform their competitors, but the difference is not significant. The hidden layer size was set to 2 and the maximum number of iterations the solver iterates was chosen to be 200. The solver for weight optimization is set to `lbfgs', a quasi-Newton optimizer. The learning rate for scheduling weight updates is set to constant. However, the generic layered structure of a neural network has proven to be time consuming. Additionally, this technique is considered a `black box' technology, and finding out why a neural network has outstanding or even poor performance is difficult \cite{noriega2005multilayer}.

%On the test set, logistic regression seems to perform  noticeably better than other models. One interesting observation is that this is the only classifier that noticeably benefited the most from hyperparameter tuning. This might be partially due to the fact that we report the different kernels of SVMs separately.

\input{tex/table_res}


\begin{figure}[ht]
\centering
\includegraphics[width=8.cm]{plots/chiang7.pdf}
\caption{ROC Learning Curves for the overall performance of each model.}
%\denes{As discussed, please replot without axis / bigger fonts. Or actually, why not do one single ROC plot (you have 4 colours anyway?}}

\label{fig:roc}
\end{figure}


Receiving operating characteristics (ROC, \figref{roc}) support the quantitative results. The kernel choice for SVM models makes a noticeable difference; the areas under the ROC curves are otherwise comparable for all other models.
%\figref{par} shows that the precision vs. recall trade-off is once again comparable for all models, except for the random forest classifier. While this seems like an interesting phenomenon, 

%ROC curves demonstrate the performance of a classification model for all classification thresholds. The closer the apex of the curve is to the upper left hand corner, the greater the model's discriminatory ability \cite{fan2006understanding}. This is plotted for each type of classifier, including the SVM model using a linear kernel. 
%For each trained model, we use it's accuracy score, F1 measure and the area under its ROC curve to evaluate overall performance\denes{do we report area as well anywhere?}.
